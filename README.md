# GPU Routing Project

## Overview
This project benchmarks IPv4 packet forwarding on CPU and GPU. It generates a routing table and a large set of synthetic packets, then runs:

- CPU serial forwarding.
- CPU parallel forwarding (OpenMP).
- GPU forwarding using longest-prefix match (LPM).
- GPU forwarding using a Bloom-filter-assisted lookup.
- Pinned vs non-pinned host memory transfers.
- A multi-batch streaming simulation to measure the benefit of reusing pinned memory.

At the end, it validates correctness by comparing CPU and GPU outputs and prints timing summaries.

## How the program works
High-level flow in `main.cpp`:

1. Build a routing table (either a small fixed table or a synthetic one with mixed /8, /16, /24 prefixes).
2. Generate `N_Packets` random packets.
3. Forward packets on CPU (serial and OpenMP parallel).
4. Convert packets from AoS to SoA for GPU processing.
5. Run GPU forwarding (LPM) and GPU Bloom-filter forwarding, both with pinned and non-pinned host memory.
6. Compare outputs for correctness and report performance.
7. Run a multi-batch test that reuses pinned memory across multiple GPU launches.

Key parameters:

- `num_packets` and `num_routes` can be passed from the command line: `./build/gpu_router [num_packets] [num_routes]`.
- `NUM_BATCHES` is still defined in `source/main.cpp` and controls the multi-batch simulation.

### Print helpers
In the project are available a `source/print_helpers.cpp` file that contain all the print function that help to visualize and describe the results of the computation.

## Requirements
### Compilers and toolchain
- **C++ compiler**: GCC 11 is recommended (required by the current CUDA configuration).
- **CUDA compiler**: NVIDIA CUDA Toolkit 11.8+ (provides `nvcc`).
- **CMake**: version 3.25 or newer.

The CMake configuration explicitly sets:

- C++ standard: C++17
- CUDA architectures: `61;75;80;86`
- CUDA host compiler: `gcc-11`

If you use a newer GCC with CUDA 11, the build allows it via:

- `--allow-unsupported-compiler` for CUDA targets

### Libraries
- OpenMP (for CPU parallel section)
- CUDA Runtime (`cudart`)

## Build system (CMake) explanation
- **CMakeLists.txt** defines the project, compiler requirements, and build targets.
- It finds OpenMP and the CUDA Toolkit, sets include paths, and builds a single executable named `gpu_router`.
- The `build/` directory is the out-of-source build folder.
- The `build/CMakeFiles/` directory is generated by CMake and contains internal build metadata (compile flags, dependency tracking, and generated build rules). You typically do not edit anything inside `CMakeFiles/`.

## Build and run (Linux)
From the project root:

1. Configure:
   - `cmake -S . -B build -DCMAKE_BUILD_TYPE=Release`

2. Build:
   - `cmake --build build -j`

3. Run:
   - `./build/gpu_router`

## Profiling on HPC@Polito (SBATCH)
To run the project on HPC@Polito using SLURM, use the provided `gpu_profile.sh` script. It wraps the executable with Nsight Systems (default) or Nsight Compute and submits the job with `sbatch`.

1. Edit the parameters in `gpu_profile.sh` (recommended before submitting):
   - `NUM_PACKETS`: number of packets to process.
   - `NUM_ROUTES`: number of routes.
   - `PROFILE_TOOL`: `nsys` (default) or `ncu`.

2. Submit the job:
   - `sbatch gpu_profile.sh`

3. Outputs:
   - The SLURM log is written to `routing_profile.log`.
   - Profiling outputs are saved in the project root with prefix `gpu_routing_nsys` or `gpu_routing_ncu`.

The script expects the executable at `./gpu_router` (project root). If you build into `build/`, copy or link the binary to the root, or adjust `EXECUTABLE` in the script.

## Notes
- If your GPU is not in the listed architectures, update `CMAKE_CUDA_ARCHITECTURES` in `CMakeLists.txt`.
- To reduce runtime during development, lower `N_Packets` in `source/main.cpp`.