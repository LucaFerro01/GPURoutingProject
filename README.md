# GPU Routing Project

## Overview
This project benchmarks IPv4 packet forwarding on CPU and GPU. It generates a routing table and a large set of synthetic packets, then runs:

- CPU serial forwarding.
- CPU parallel forwarding (OpenMP).
- GPU forwarding using longest-prefix match (LPM).
- GPU forwarding using a Bloom-filter-assisted lookup.
- Pinned vs non-pinned host memory transfers.
- A multi-batch streaming simulation to measure the benefit of reusing pinned memory.

At the end, it validates correctness by comparing CPU and GPU outputs and prints timing summaries.

## How the program works
High-level flow in `main.cpp`:

1. Build a routing table (either a small fixed table or a synthetic one with mixed /8, /16, /24 prefixes).
2. Generate `N_Packets` random packets.
3. Forward packets on CPU (serial and OpenMP parallel).
4. Convert packets from AoS to SoA for GPU processing.
5. Run GPU forwarding (LPM) and GPU Bloom-filter forwarding, both with pinned and non-pinned host memory.
6. Compare outputs for correctness and report performance.
7. Run a multi-batch test that reuses pinned memory across multiple GPU launches.

Key parameters (edit in `source/main.cpp`):

- `N_Packets`: total packets to process.
- `NUM_ROUTES`: number of routing entries.
- `NUM_BATCHES`: number of batches in the streaming simulation.

### Print helpers
In the project are available a `source/print_helpers.cpp` file that contain all the print function that help to visualize and describe the results of the computation.

## Requirements
### Compilers and toolchain
- **C++ compiler**: GCC 11 is recommended (required by the current CUDA configuration).
- **CUDA compiler**: NVIDIA CUDA Toolkit 11.8+ (provides `nvcc`).
- **CMake**: version 3.25 or newer.

The CMake configuration explicitly sets:

- C++ standard: C++17
- CUDA architectures: `61;75;80;86`
- CUDA host compiler: `gcc-11`

If you use a newer GCC with CUDA 11, the build allows it via:

- `--allow-unsupported-compiler` for CUDA targets

### Libraries
- OpenMP (for CPU parallel section)
- CUDA Runtime (`cudart`)

## Build system (CMake) explanation
- **CMakeLists.txt** defines the project, compiler requirements, and build targets.
- It finds OpenMP and the CUDA Toolkit, sets include paths, and builds a single executable named `gpu_router`.
- The `build/` directory is the out-of-source build folder.
- The `build/CMakeFiles/` directory is generated by CMake and contains internal build metadata (compile flags, dependency tracking, and generated build rules). You typically do not edit anything inside `CMakeFiles/`.

## Build and run (Linux)
From the project root:

1. Configure:
   - `cmake -S . -B build -DCMAKE_BUILD_TYPE=Release`

2. Build:
   - `cmake --build build -j`

3. Run:
   - `./build/gpu_router`

## Notes
- If your GPU is not in the listed architectures, update `CMAKE_CUDA_ARCHITECTURES` in `CMakeLists.txt`.
- To reduce runtime during development, lower `N_Packets` in `source/main.cpp`.